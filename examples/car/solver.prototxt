net: "train_confusion.prototxt"
test_initialization: false
# lr for fine-tuning should be lower than when starting from scratch
base_lr: 0.00001
lr_policy: "multistep"
gamma: 0.8
# stepsize should also be lower, as we're closer to being done
stepvalue: 10000
display: 50
max_iter: 10000
momentum: 0.9
momentum2:0.999
weight_decay: 0.0002
snapshot: 1000
snapshot_prefix: "run/model"
solver_mode: GPU
type:"Adam"
